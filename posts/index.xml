<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on hwy的小窝</title><link>https://baymaxhwy.github.io/posts/</link><description>Recent content in Posts on hwy的小窝</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 13 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://baymaxhwy.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>深入理解 MVCC</title><link>https://baymaxhwy.github.io/2023/mvcc/</link><pubDate>Mon, 13 Feb 2023 00:00:00 +0000</pubDate><guid>https://baymaxhwy.github.io/2023/mvcc/</guid><description>前言 印象中第一次看见 MVCC 是在《DDIA》这本书上，书中 MVCC 是作为解决 Non-repeatable Read 问题的解决方法提出的，当时觉得非常nb，但是现在细想来其实对其中的实现细节并不清晰，如果让我来实现一个 MVCC 无从下手。而且既然 MVCC 可以解决 Non-repeatable Read 问题，那它能不能解决其他隔离性的问题？如何做到？最近阅读到了《An Empirical Evaluation of In-Memory Multi-Version Concurrency Control》这篇论文对 MVCC 的实现有了更深入的了解，这篇文章就是我结合论文和自己的一些理解来尝试剖析一下 MVCC。
MVCC 简介 Multi-Version concurrency control(MVCC) 从名字上看就是一种并发控制算法用以控制并发读写单一对象时的行为，尽可能减少竞争并保证一定隔离性。实现的基本原理是同时维护同一个逻辑对象（Logical Object） 不同时间段的逻辑对象，而一个逻辑对象在某个时间段的数据被称为一个 Version ，当对一个对象进行读/写操作时会根据相应的规则返回特定的 Version ，如果对某个对象完成了更新操作并不会直接将更新应用在其访问的 Version 对象上，而且重新创建一个新 Version 对象。
MVCC 实现 在论文中将 MVCC 的关键组件分为四个：Protocol、Version Storage、GC（Garbage Collection）和 Index，而 MVCC 所管理的逻辑对象可以是一个事务（Transactions）或者数据表的一行数据（Tuple/Record），一般都是以 tuple 为操作对象，下面都以 tuple 来介绍。tuple 的存储格式如下（不同 protocol 会有所区别） 1. Protocol MVTO（Timestamp Ordering） MVTO 为每个事务分配一个唯一的时间戳($T_{id}$)，它会中止所有试图读/更新已经被设置 write lock 的 version tuple（当 txn-id 不等于 0 或者自己的 $T_{id}$ 时表示有其他事务持有 write lock）的事务。MVTO 协议的 tuple header 如下： 对 tuple A 进行读操作： a.</description></item><item><title>Raft总结</title><link>https://baymaxhwy.github.io/2020/mit6.824-raft/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://baymaxhwy.github.io/2020/mit6.824-raft/</guid><description>前言 这里是对MIT6.824课程中有关Raft部分的一些总结（包括一些个人的理解以及Lab2的实现思路等）。首先还是很感谢MIT的大牛们能把这门神课开源出来，让更多的人能更加系统的学习分布式系统（搭配《DDIA》效果更佳！）。
简介 因为需要遵守 Collaboration Policy 不能公布全部代码，主要是思路和伪代码。
Raft是一个用于管理副本log的共识算法，它的功能类似于Paxos，但是比Paxos更简单（相对而言），更容易让人去理解学习，也容易在现实中实现。Raft主要有这几部分：领导选举、日志复制、集群变更、日志持久化、快照，本文只讨论与Lab2有关的部分。
共识算法&amp;mdash;详细可以去看书 在6.824课程和《Raft》论文中对共识的阐释没有系统具体的阐释，这里主要是引用《DDIA》中的关于共识的部分内容：
共识问题是分布式计算中最重要也是最基本的问题之一。共识其实就是让几个节点就某件事情达成一致。这样表面上看去不是太难，但是因为分布式系统本身就是不可靠的，存在各种问题、失败、故障（网络故障、进程暂停、时钟问题等），想让多个节点到达共识其实是存在很大的难度。
共识算法需要满足一些性质：
协商一致性（Uniform agreement）：所有节点都接受相同的决议 诚实性（Integrity）：所有节点不能反悔，即对一项提议不能有两次决定 合法性（Validity）：如果决定了值v，则v一定由某个节点所提议 可终止性（Termination）：节点如果不崩溃则最终一定可以达成协议 协商一致性和诚实性决定了共识算法的核心思想：决定一致的结果，一旦决定，就不能改变。合法性（有效性，书里面会混用，属于翻译问题）则是排除一些无意义的方案，例如值为空（NULL）。前三个主要是保障了安全性（Safety），而可终止性则是引入了容错的思想。
2PC，两阶段提交也是共识算法但是不具有容错性，因为它的leader（独裁者、协调者）是人为指定的，也就没有可终止性，如果协调者故障则系统会一直原地空转
最著名的容错共识算法包括：VSR、Paxos、Raft和Zab（Zookeeper），他们大都不是直接使用上面的形式化描述。相反，他们是决定了一系列值，然后采用全序关系广播算法。
具体实现（思路和伪代码） 领导选举（Lab2A） 领导选举其实就是一次共识的过程，需要一个Candidate节点发起election让其与的节点来进行投票（vote）。
首先这里先介绍一下Raft中每个节点能够扮演的角色：
Follower：只负责接收Leader、Candidate的请求来进行响应，自身无法主动发起任何请求，但是会有一个election time，如果到期则会成为Candidate节点 Candidate：自身的term+1，重置election time，启动一个election vote活动，向集群中的所有节点发送vote请求，如果能在下一次election time到期之前获得大多数（Majority）节点的投票则会成为Leader（Candidate自己会投给自己），否则会开启新一轮的election Leader：每个term最多只有一个Leader，Leader会定时向集群中的节点发送心跳包来阻止新一轮的选举，同时会接受client的请求并复制到Follower节点 领导选举的整个流程：
首先所有节点初始化为Follower状态，并且设置election time（注意为了防止出现多个Candidate 的情况，需要在指定区间内随机time的值）
当time out时某个Follower会成为Candidate，开启一轮选举，如果获得大多数投票则选举成功，成为Leader，否则等待下一次time out开启新一轮选举。
Leader会以更短（比election time）的时间间隔向其他节点发送心跳包防止进行新选举
基本上需要按照论文中Figure2中的表示进行编写代码，不然容易出bug。这里需要注意的一点：
代码主要分为三部分：Election time超时机制，Candidate发起选举，RequestVote RPC处理
Election time超时机制。 首先每个Raft实例都会启动一个attemptElection的goroutine
func (rf *Raft) attemptElection() { for !rf.killed() { timeout := getRandTime() // 预定超时时间 time.Sleep(timeout) if time.Since(rf.lastHeartMsg) &amp;gt; timeout &amp;amp;&amp;amp; rf.state != Leader { rf.</description></item></channel></rss>